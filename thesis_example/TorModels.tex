
\chapter{Existing Tor Simulations}

\section{ExperimenTor}
ExperimenTor \citep{exptorwpaper} is a Tor simulation toolkit built on top of the ModelNet \citep{modelnetwpaper} network emulation platform. At least two computers on the same intranet are required to run ExperimentTor - one or more FreeBSD \emph{core routers} to simulate the network, and one or more Linux \emph{edge nodes} to run client processes like Tor relays and clients, BitTorrent clients, and HTTP servers. As an extension to ModelNet, ExperimenTor inherits the ability to scale to an arbitrarily sized computing cluster, yielding the potential for a very large simulated Tor networks. ExperimenTor is a real time simulation, and is capable of running the Tor software completely unmodified.

To date, ExperimenTor has been used in two research projects on Tor. \citet{defenestrator} use ExperimenTor to test new techniques for congestion and flow control. In a separate paper, \citet{pathlesstraveled} also use ExperimentTor to evaluate a new approach to multipath circuit construction and stream splitting.

In ModelNet, a network topology is specified in a Graph Modeling Language file. Edges in the graph, representing network links, may be annotated with values for bandwidth, latency, and packet drop rate. For very large topologies, ModelNet may optionally ``distill'' the network graph into a simpler form that is less computionally taxing to simulate.

Client processes like Tor run in \emph{virtual nodes}, each of which represents one machine connected to the virtual network. Depending on how well-provisioned it is, one edge node may run many distinct virtual nodes. Each virtual node is assigned a unique IP address in the 10.0.0.0/8 address space. To ensure that outgoing TCP packets are correctly marked with their associated virtual address, system calls for socket creation and name resolution - \texttt{bind}, \texttt{connect}, \texttt{gethostbyaddr}, and \texttt{gethostbyname}, for example - are intercepted at runtime.

To properly simulate effects like cross traffic and congestion, ModelNet models network links as \textit{pipes}. A pipe is a packet queue with a ``leaky bucket'' at its entrance. The leaky bucket analogy stems from the way incoming packets are handled. Packets accumulate at the beginning of the pipe and ``leak'' into it at a constant rate. As long as the average packet rate stays below the pipe's bandwidth rating, no packets are dropped. If the bandwidth rating is exceeded, however, the entrance ``overflows'' and packets are lost. Since pipes are shared between virtual nodes, a burst of packets from one node can thus cause another's packets to be dropped shortly afterward. This behavior is consistent with that of a real network.

ExperimenTor automates the process of building a functional Tor network within ModelNet. Using the included configuration tool, a researcher may use the live Tor consensus document to generate a realistic set of Tor relays and clients. The configuration tool maps router bandwidth measurements in the consensus to virtual nodes. To assign reasonable latencies to network links, the tool samples from the King network latency dataset \citep{king}. At the beginning of an experiment, private directory servers are created, and their public/private keys are distributed to all Tor routers and clients in the virtual network. With routers, clients, and directory authorities instantiated, the virtual network then functions as in the same manner as the live one.

\citet{exptorwpaper} report running a simulated network of 1000 Tor nodes with one ModelNet core node, one edge node, and a 1 Gbps network switch. Since Tor has an estimated 250,000 clients daily, however, the problem of how to best ``scale down'' the distribution of clients remains an open question. For relays, ExperimenTor's configuration tool maintains the same proportion of guard, middle, and exit nodes, while scaling all node bandwidths by a common factor. While this is a reasonable approach, \citet{exptorwpaper} note that its effects on the simulated network's ``realism'' have not been thoroughly studied.


\section{Shadow}
Shadow \citep{shadowwpaper} is a network emulation platform built for running Tor simulations. Unlike ModelNet, Shadow is easily run on one well-provisioned computer. Shadow is a discrete event simulator, meaning that all events in a network experiment occur in virtual time. Client applications are encapsulated by plugin libraries that implement a standard interface for communicating with the virtual network. Shadow may run multiple instances of a client application by dynamically managing each one's memory in a strategy similar to kernel context switching.

Shadow has been used in three Tor research projects. \citet{shadowwpaper} use Shadow to experiment with a recently integrated replacement for Tor's round-robin circuit scheduling algorithm. \citet{methmodelnetwork} use Shadow and Experimentor to validate the accuracy of a Tor network model. \citet{throttleparas} use Shadow to test three algorithms for throttling bulk transfers.

In Shadow, a network topology is specified as a linked set of geographic clusters. Each cluster is given values for upstream/downstream bandwidth and packet loss rate. Links between clusters are given values for latency and jitter (variation in packet delay). Client applications like Tor relays and web servers may then be attached to each cluster. Geographic clustering is used to scale the internet down to a manageable size for simulation. \citet{methmodelnetwork} argue that this is appropriate because geographic clustering most closely resembles the structure of the live internet. Many properties of network vertices and edges are dictated by their geographic location.

Shadow includes implementations of a bulk downloader, a simple web server, and a simulated web browser. Like ExperimenTor, Shadow's virtual network is built on ``pipes'' with leaky buckets at their entrance \citep{shadowwpaper}. These pipes are shared amongst client applications, resulting in realistic congestion and cross traffic effects.

To communicate with Shadow's virtual network, a client application is encapsulated by a Shadow plugin. A plugin is a library, written independently of the client, that implements a set of callbacks defined by Shadow. These callbacks allow Shadow to perform tasks such as creating new application instances, assigning IP addresses, and sending/receiving packets in a uniform manner across plugins. To integrate correctly into the simulation, a plugin's encapsulated application must be single-threaded and non-blocking.

In addition to implementing a set of callbacks, a plugin must register pointers to all of its application's variable state with Shadow. This step is what allows Shadow to run multiple instances of an application simultaneously. When the simulation needs to communicate with a running instance, that instance's variable state is loaded into the appropriate memory locations and control is passed to it. Once communication is finished, the application relinquishes control and its variable state is moved to another, inactive memory location. Shadow may then load another instance's state and transfer control.

Shadow runs Tor by wrapping it with the Scallion plugin. To register Tor's variable state with Shadow, Scallion uses standard binary utilities like \texttt{objcopy}, \texttt{readelf}, and \texttt{nm} to scan, rename, and globalize Tor symbols. Registration code for these globalized symbols is then generated before compilation and included in Scallion. To further adapt Tor without modifying its source code, Scallion uses function interposition to replace selected pieces of functionality with appropriate substitutes at runtime. Using the latter technique, Scallion intercepts all of Tor's socket system calls with its own virtual socket implementation. The virtual socket library packages outgoing data into packet objects, which are then pushed to the appropriate packet queue in the simulated network.

Shadow comes with a global network topology created by \citet{methmodelnetwork} in their Tor modeling study. Values for each country's upstream/downstream bandwidth and packet loss are taken from the Ookla Net Index dataset [\citeauthor{ookla}]. Latency and jitter estimates are derived from measurements on the iPlane latency estimation service [\citeauthor{iplane}]. In addition, Shadow includes four scaled-down Tor router/client distributions derived using \citet{methmodelnetwork}'s \emph{best fit sample} algorithm. These distributions range from 20 routers and 200 clients to 250 relays and 2500 clients \citep{shadowwiki}. The former can run with less than 4 GiB RAM, while the latter requires nearly 64 GiB.
