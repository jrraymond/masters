\documentclass[12pt]{article}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mdwlist}
\usepackage{graphicx}
\usepackage[font=small,labelfont=bf]{caption}

\DeclareGraphicsExtensions{.pdf,.png}

\input comp321-listings
\lstset{style=pseudocode}

\title{Oates et al.}
\author{Julian Applebaum}
\begin{document}
\citet{oates} detail a procedure to cluster a set of time series using HMMs and dynamic time warping. Like \citet{smyth}, the final products of \citeauthor{oates}'s procedure are a partition on the set of time series and a set of HMMs, each of which is trained to one cluster. Dynamic time warping is used as the similarity function in an initial bootstrap round of clustering. This bootstrap round is necessary because HMM clustering is sensitive to a na\"{i}vely chosen initial partition. HMM clustering is used to simultaneously refine the bootstrap partition and train a corresponding set of HMMs.

Dynamic time warping \citep{oates} is a similarity function between two time
series of equal or unequal length. Given two time series $W = w_1, w_2 \dots, w_{L_w}$ and $Y = y_1, y_2 \dots, y_{L_y}$, DTW finds the warping of $W$ onto $Y$ that minimizes the distance between the two series. Given $t_w \in {1, 2, \dots, L_w}$ and $t_y \in {1, 2, \dots, L_y}$, a warping of $W$ onto $Y$ is a function
\[\phi(t_w) = t_y \]
 that maps elements of $W$ onto elements of $Y$. Intuitively, $\phi(t_w)$ ``stretches'' $W$ so that each data point $w_{t_w} \in W$ is aligned on the time axis with a point $y_{t_y} \in Y$. In DTW literature, $\phi$ is typically subject to two constraints:
\begin{description*}
	\item[Boundaries:] $\phi(1) = 1, \phi(L_W) = L_Y$.
	\item[Monotonicity:] $t_i \geq t_j \Rightarrow \phi(t_i) \geq \phi(t_j)$
\end{description*}
 Figure~\ref{dtwgraphic} illustrates two possible warpings of $W$ onto $Y$ --- note how in each one, as a data point is shifted right (left), points to its right (left) contract, while points to its left (right) expand to fill the void in time. To find the optimal warping $\phi'$, DTW uses dynamic programming to minimize the sum
 \[D_\phi = \displaystyle\sum\limits_{t_w = 1}^n d(w_{t_w}, y_{\phi(t_w)})\]
where $d$ is a distance measure between two individual data points (eg. Euclidean distance).

\begin{figure}[t]
	\centering
	\includegraphics[width=400px]{dtw}
	\caption{Two time series, $W$ and $Y$, (the leftmost column) and two possible warpings of
	$W$ onto $Y$ (the middle and rightmost columns) \citep{oates}.}
	\label{dtwgraphic}
\end{figure}

To perform the bootstrap clustering, DTW is used as the similarity function in a hierarchical, agglomerative clustering algorithm. At each iteration of the algorithm, the pair of clusters with the minimum distance between them is merged. To prevent the algorithm from merging all the time series into one cluster, clusters whose mean intracluster distance is significantly different from the mean intercluster distance are not merged. Significance is determined by a t-test.

\begin{figure}
	\begin{lstlisting}
	fun refineCluster(S: set of time series)
	begin
		$S_0$  <- $\{\}$
		$S_0'$ <- $S$
		$\lambda$ = HMM()

		while $S_0 \neq$ $S_0'$
			$S_0$ <- $S_0'$
			$\lambda$ <- trainMarkov($S_0$)
			$S_0'$ <- $\{ s \in S_0: \textrm{accepts}(\lambda, s) \}$
		endw

		return ($S_0$, $\lambda$)
	end
	\end{lstlisting}
	\caption{Pseudocode for \citeauthor{oates}'s cluster refining algorithm}
	\label{fig:refinecluster}
\end{figure}

As part of their HMM clustering algorithm, \citeauthor{oates} define the notion of a HMM ``accepting'' or ``rejecting'' a sequence. A HMM $\lambda$ accepts a sequence $O$ if $L = log(p(O|\lambda))$, the log-likelihood that $\lambda$ generated $O$, is sufficiently high. To determine how high $P(O|\lambda)$ must be, a large sample of sequences $S = s_1 \dots s_n$ is first generated from $\lambda$. With this sample, an empirical probability distribution $\theta$ of log-likelihoods is generated by calculating $\textrm{log}(P(s|\lambda))$ for every $s \in S$. The null hypothesis $h_0$ that $L$ was drawn from $\theta$ is then tested. The authors do not mention a preferred confidence interval, so we assume the standard 95\%. If $h_0$ is true, then $\lambda$ accepts $O$; otherwise, it is rejected.

\begin{figure}
	\begin{lstlisting}
	$P$  <- ClusterDTW($S$)
	$P'$ <- $\{\}$
	$R$  <- $\{\}$

	// refine the DTW clusters with the HMM algorithm
	for $c$ in $P$
		$(c', \lambda)$ <- refineCluster($c$)
		$P'$ <- $P' \cup \{(c', \lambda)\}$
		$R$  <- $R \cup (c \setminus c')$
	endf

	// attempt to add back stray sequences
	for $r$ in $R$
		for $(c', \lambda)$ in $P'$
			if $\textrm{accepts}(\lambda, r)$
				$c'$ <- $c' \cup \{r\}$
				$R$  <- $R \setminus \{r\}$
			endif
		endf
	endf

	// clean up any remaining sequences
	while |R| > 0
		$(c, \lambda)$ <- refineCluster($R$)
		$P'$ <- $P' \cup {(c, \lambda)}$
		R  <- $R \setminus c$
	end

	\end{lstlisting}
	\caption{Pseudocode for \citeauthor{oates}'s HMM clustering algorithm}
	\label{fig:hmmcluster}
\end{figure}

With the concept of acceptance established, the HMM clustering algorithm is now outlined. \citeauthor{oates} begin by defining an algorithm for refining a preexisting cluster. Pseudocode for this algorithm is presented in Figure ~\ref{fig:refinecluster}. At each iteration of the while loop, a new HMM is trained on the cluster $S_O$. Any sequences rejected by this HMM are then removed from the cluster. The loop iterates until $S_0$ stops changing.

Pseudocode for the DTW+HMM clustering algorithm is presented in Figure ~\ref{fig:hmmcluster}. First, the set of sequences is clustered using DTW. Each cluster is then refined, and thus has a
HMM trained on it. The algorithm then attempts to add back any sequences that were rejected by the refinement process. Finally, any remaining sequences are clustered by repeatedly applying the refinement algorithm.

The authors performed two experiments to evaluate the performance of DTW+HMM clustering - one with synthetic data, and one with robot sensor readings. DTW+HMM performed well on the synthetic data with the exception of two extraneous clusters generated by the ``clean up'' phase of the algorithm. DTW+HMM performed poorly on the robot sensor data --- both DTW and HMM clustering alone were better. The authors note that knowing the number of HMM states ahead of time may improve performance
on this data set.
\bibliography{bibliography}{}
\bibliographystyle{abbrvnat}
\end{document}
