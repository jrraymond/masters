\chapter{Clustering}

\section{Overview}
Clustering is the process of partitioning a set of data points into two or more groups such that elements in the same group are qualitatively similar, while elements in separate groups are distinct. In the machine learning literature, clustering is classified as an unsupervised learning method because it does not make use of pre--classified data points. Formally, given a set of input vectors $X = \{x_1, x_2, \dots, x_n \}$, clustering creates a $k$-partition $C = \{c_1, c_2, \dots\, c_k\}$ obeying three properties \citep{xuwunsch}:
	\begin{itemize}
		\item $c_i \neq \emptyset$ (no cluster may be empty)
		\item $\bigcup_{i=1}^k c_i = X$ (no orphan data)
		\item $c_i \cap c_j = \emptyset, i, j = 1, 2, \dots, k, i \neq j$ (no overlap between partitions)
	\end{itemize}
Clustering has a wide range of applications in data mining. In market research, clustering is used to segment customers into meaningful subgroups for targeted advertising \citep{gan2007}. In bioinformatics, it is used to group gene expression data into groups with similar expression patterns. Examples abound in other fields.

To cluster a dataset, two components are typically needed: a dissimilarity or similarity function, and a clustering algorithm. A \textit{dissimilarity function} $d: X \times X \rightarrow \mathbb{R}$ is a function that expresses how dissimilar two data points are. $d$ must obey two axioms:
	\begin{itemize}
		\item $d(x_i, x_j) = d(x_j, x_i) \quad \forall i, j$ (Symmetry)
		\item $d(x_i, x_j) \geq 0 \quad \forall i, j$ (Positivity)
	\end{itemize}
If $D$ also satisfies the two axioms below, it is referred to as a \textit{distance metric}.
	\begin{itemize}
		\item ${d(x_i, x_j)} \leq d(x_i, x_k) + d(x_k, x_j)$ (Triangle Inequality)
		\item $d(x_i, x_j) = 0 iff x_i = x_j$ (Reflexivity)
	\end{itemize}
A similarity function $s: X \times X \rightarrow \mathbb{R}$ must obey the same Symmetry and Positivity axioms. To qualify as a metric, it must obey these additional axioms:
	\begin{itemize}
		\item $s(x_i, x_j)S(x_j, x_k) \leq [s(x_i, x_j) + s(x_j, x_k)]s(x_i, x_k)$
		\item $s(x_i, x_j) = 1 iff x_i = x_j$ (Reflexivity)
	\end{itemize}
\section{The $k$-Means Algorithm}
The k-means algorithm is one of the simplest and most widely used clustering algorithms \citep{gan2007}. It was first described by \citet{macqueen67}. Given a $n$-dimensional dataset $X \subseteq \mathbb{R}^n$ and a fixed number $k$, $k$-means  partitions $X$ into $k$ clusters. k-means begins by first randomly\footnote{The initial partition may be created in other ways, but it is typically done randomly.} partitioning $X$ into $k$ initial clusters. Each cluster $C$ has a \textit{centroid} or \textit{cluster prototype} $\mu(C)$ that represents $C$'s center. This is typically the $n$-dimensional vector whose $j$th coordinate is the mean of the $j$th coordinate of the vectors in $C$.

The partition is iteratively refined as follows: first, for each data point $x \in X$, the distance between $x$ and the centroid of each cluster is calculated. Each $x$ is then reassigned to the cluster whose centroid it is closest to. The centroids are then recomputed for any clusters that have changed. This process is repeated until either the clusters stop changing, a maximum number of iterations is achieved, or the change in the error function
\[ E = \sum_{i=1}^k \sum_{x \in C_i} D(x, \mu(C)) \]
is less than a threshold $\epsilon$\citep{gan2007}. In Scikit--learn \citep{scikit-learn}, a machine learning library for Python, the $k$--means function sets $\epsilon=0.0001$ by default, and terminates after 300 iterations unless otherwise specified.

$E$ is an example of a cluster quality metric. A low $E$ implies that $X$ has been partitioned into \textit{compact} and \textit{well-separated} clusters. Conversely, a high $E$ indicates that one or more clusters contain inappropriately heterogeneous data. In general, cluster quality metrics give us a concrete way to assess how effectively a given algorithm and dissimilarity function partitions a data set. A wide variety of metrics exist for different algorithms and data types. \citet{gan2007} provides an extensive survey of available metrics.

Pseudocode for $k$--means is presented in figure ~\ref{fig:kmeans}.
\begin{figure}
	\begin{lstlisting}
	fun kmeans(X:dataset, k: int, $\epsilon$: real)
	begin
		clusters <- randPartition($X$, $k$)
		terminate <- false
		while not terminate
			centroids <- %*centroids for clusters*)
			curErr <- E(clusters)
			for x in X
				$C$ = %*the cluster that $x$ is currently in*)
				$C'$ = $\textrm{arg min}_{1 \leq i \leq k} d(x, c_i)$
				if $C \neq C'$
					%*Move $x$ to $C$*)
					didChange = true
				else
					didChange = false
				endif
			endf
			terminate <- didChange and curErr - E(clusters) > $epsilon$
		endw
		return clusters
	end
	\end{lstlisting}
	\caption{Pseudocode for the $k$-Means algorithm.}
	\label{fig:kmeans}
\end{figure}
$k$--means' most desirable characteristic is its speed. Each iteration requires $O(|X|kn)$ calculations \citep{phillips02}. Since $k$ and $n$ are typically small constants, it may run in essentially linear time with respect to $|X|$. This is significantly faster than most other clustering algorithms, making $k$--means well suited for large datasets. $k$--means does have well--documented drawbacks, though:
	\begin{itemize}
		\item It only works well for clusters in which the data points tend to clump into hyperspheres, the $n$-dimensional generalization of spheres.
		\item There is no efficient and universal method for choosing the correct $k$ or creating the initial partition. For a given $k$, a poor initial partition may result in misplaced data points, yielding a sub-optimal $E$. If $k$ is too low, then qualitatively distinct data points may be forced into the same cluster; if it is too high, then similar points may be incorrectly separated from one another.
		\item Since every object is forced into a cluster, it is sensitive to outliers. PAM, a variation on k-means, addresses this by using the median instead of the mean as the cluster prototype \citep{kaufman90}. Since the median is a robust statistic, this prevents the cluster centroids from being adversely affected by outliers.
	\end{itemize}
\section{Hierarchical Clustering}
Unlike $k$-means, which creates a single partition on $X$, hierarchical, agglomerative clustering builds a nested tree of partitions. The algorithm begins by dividing $X$ into $|X|$ singleton clusters -- the leaves of the tree in Figure ~\ref{fig:hierarchdiag}. At each iterative step, the two closest clusters are merged. The algorithm continues to merge clusters until there are only two left. To derive a $k$ partition, the tree is cut at the appropriate level.
\begin{figure}
	\centerline{\includegraphics[scale=1.0]{figures/hierarchical.png}}
	\caption{Illustration of agglomerative vs. divisive clustering.}
	\label{fig:hierarchdiag}
\end{figure}
\begin{figure}
	\begin{lstlisting}
		fun agglomerative(X: dataset)
		begin
			n <- $|X|$
			partitions <- []
			P <- $\{\{x\} : x \in X\}$
			while $|P| > 2$
				partitions <- P::partitions
				i, j = $\textrm{arg min}_{1 \leq i,j \leq n, i \neq j} D(P_i, P_j)$
				P <- $(P - \{P_i, P_j\}) \cup \{P_i, P_j\}$
				n <- n - 1
			endw
			return partitions
		end
	\end{lstlisting}
	\caption{Pseudocode for hierarchical, agglomerative clustering.}
	\label{fig:hierarchcode}
\end{figure}

To determine which clusters to merge at each step, a dissimilarity function $D$ defined on clusters instead of data points is needed. Two such measures are single linkage and complete linkage \citep{gan2007}. Given two clusters $C_1$ and $C_2$, single linkage defines the dissimilarity to be the pairwise dissimilarity between the two closest objects in $C_1$ and $C_2$:
\[D_s = \textrm{min} \{D(x_1, x_2): x_1 \in C_1, x_2 \in C_2\}\]
For this reason, it is also referred to as the nearest neighbor method. In complete linkage, $D$ is the pairwise distance between the furthest two elements in the two clusters:
\[D_c = \textrm{max} \{D(x_1, x_2): x_1 \in C_1, x_2 \in C_2\}\]
\citeauthor{gan2007} surveys other available cluster dissimilarity functions.

Hierarchical, agglomerative clustering's greatest advantage is that $k$, the number of clusters, does not need to be known a priori. Instead, the researcher may examine the tree of partitions and choose the optimal level to cut at \citep{xuwunsch}. The optimal level may be found by visualizing the tree in a similar manner to ~\ref{fig:hierarchdiag} or by calculating a cluster quality measure such as $E$ at each level. A criticism of classical hierarchical algorithms is that they are sensitive to outlier data points. This is because once a data point is assigned to a cluster, it is not reconsidered at higher levels of the tree. If a data point is misplaced at an early stage of the algorithm, it will not be corrected. Classical hierarchical algorithms also run in at best $O(N^2)$ time, since at the bottom of the tree $|X|$ singleton clusters must be pairwise compared to one another. CURE, ROCK, Chameleon, and BIRCH are variants of agglomerative clustering developed in the late 1990's and early 2000's that seek to address some of the classical algorithm's weak points\citep{xuwunsch}. They improve upon its complexity and add robustness against outliers via techniques such as random sampling and alternative representations of the data's hierarchical structure.

\section{The HMM Clustering Algorithm}
The methods discussed thus far are normally used to cluster vector data. To model Tor clients, however, we need to cluster a set of time series. The most challenging aspect of clustering time series is defining a meaningful dissimilarity function between two series. Qualitatively, two time series may be considered ``close'' if their observation distributions are similar and they display similar temporal patterns. Using descriptive statistics such as those previously discussed, the observation distributions of two series may be meaningfully compared. Comparing temporal dynamics, however, is a more difficult problem. For uniform length series, the Pearson correlation is a reasonable method of comparison; however, it is not defined on variable length series. To compare variable length series, more advanced methods are needed.

\citet{xuwunsch} survey the three most common approaches to clustering variable length time series. \textit{Sequence similarity} algorithms use a dissimilarity function defined directly on two series, then cluster with a standard procedure like $k-means$ or the agglomerative algorithm. Levenshtein/edit distance is a dissimilarity function commonly used for this purpose \citep{sankoff99}. \textit{Indirect clustering} algorithms summarize the characteristics of a time series in a fixed-length \textit{feature vector} $v$ which may then be used as input to a standard clustering procedure. Methods for constructing $v$ vary based on which qualitative aspects of a time series the researcher is most interested in. \textit{Statistical sequence clustering} partitions and models a set of time series by treating it as the output of a set of stochastic processes. Since these methods directly address the issue of modeling, they are particularly well suited to our research.

\citet{smyth} presents a statistical sequence clustering algorithm, HMMCluster, for partitioning and modeling a set of time series using HMMs. Given a set of series $X$ and constants $k$ and $m$, HMMCluster yields both a $k$--partition of $X$ and a $k*m$ state ``composite'' HMM faithful to $X$'s cluster structure. HMMCluster proceed as follows: first, a HMM is trained on each individual series. The series are then clustered using a dissimilarity function based on log--likelihood values. HMMs are next trained on the clusters and combined into one composite model, which is finally refined with the Baum-Welch algorithm.

Pseudocode for HMMCluster is presented in Figure ~\ref{fig:hmmcluster}. To train a $m$ state HMM $\lambda_i$ on a time series $x_i$, $\lambda$'s parameters are initialized in a ``default'' manner with the function initHMM. Pseudocode for initHMM is presented in Figure~\ref{fig:inithmm}. initHMM first assigns $\lambda$ a uniform transition matrix. It then uses $k-means$ to cluster $x_i$'s observations into $m$ clusters $c_1, c_2, \dots,c_m$, each of which covers a slice of $x$'s emission distribution. Each state $Q_i$ is assigned a Gaussian emission distribution with mean $\mu(c_i)$ and standard deviation $\sigma(c_i)$. $\lambda$ is then reestimated on $x_i$ with the Baum-Welch algorithm.
\begin{figure}
	\begin{lstlisting}
	fun initHMM(s: sequence, m: int)
	begin
		$\lambda$ <- HMM()
		C <- kmeans(s, m)
		$\lambda.\textrm{B}$ <- $[(\mu_c, \sigma_c) : c \in C]$
		for i in [1, m]
			for j in [1, m]
				$\lambda.\textrm{A}_{ij}$ <- 1/m
			endf
		endf
		return $\lambda$
	end
	\end{lstlisting}
	\caption{Pseudocode for \citet{smyth}'s ``default'' HMM initialization.}
	\label{fig:inithmm}
\end{figure}
The distance between two series is $x_i, x_j$ now defined in terms of log--likelihoods:
\[d(x_i, x_j) = 1/2(L(x_i|\lambda_j) + L(x_j|\lambda_i))\]
Using $L$, $X$ is partitioned into $k$ clusters. \citeauthor{smyth} suggests using the agglomerative algorithm with complete linkage, but notes that any applicable clustering algorithm is a valid choice. With initHMM, a new $m$ state HMM $\lambda_{C}$ is then trained on each cluster $C$. \citeauthor{smyth} does not specify how to generalize initHMM from a single sequence to a cluster, so we assume that at the ``default'' initialization step, the series in $C$ are concatenated into one joint series whose observations are then clustered.

In HMMCluster's last step, all $k$ HMMs are now combined into one composite model $\lambda'$. Let $A^1 \dots A^sk$ be the transition matrices of $\lambda_{C_1} \dots \lambda_{C_k}$. $A_1 \dots A_k$ are placed into the block diagonal transition matrix:
\[A' =
\begin{bmatrix}
A_1 		& 0 		 & \cdots	& 			& 0			\\
0			& \ddots	 & 			& 0			& \vdots	\\
\vdots		&			 & A_j		&			&			\\
			& 0			 & 			& \ddots	& 0			\\
0			& \cdots 	 & 			& 0 		& A_k		\\
\end{bmatrix}
\]
$A'$s block diagonal structure isolates the $\lambda_{C_i}$'s from one another. Any time series emitted by $O$ is the product of one distinct $\lambda_{C_i}$.

The composite initial state distribution $\pi'$ is constructed as follows. Let $w({C_i}) = |C_i|/|X|$ be the weight of cluster $C_i$ in the composite HMM and $\pi^i$ be the initial state distribution of model $\lambda_{C_i}$. For $i = i \dots k$, the weighted distribution $[w(C_i)*\pi^i_1, \dots w(C_i)]$ is appended to $\pi'$.

The net effect of this construction is that $\lambda'$ behaves as though it were $k$ separate HMMs, each of which has probability $w({C_i}$ of being chosen to emit a series. An ancillary advantage of this approach is that evaluating the log--likelihood of $\lambda'$ on a a set of sequences is much more straightforward than it would be given $k$ individual models.
\begin{figure}
	\begin{lstlisting}
	fun HMMCluster(S: list of sequence, m: int, k: int)
	begin
		Z <- Matrix($|S|$, $|S|$) // the distance matrix
		M <- {} // the initial HMMs, one per sequence
		for s in S
			$\lambda$ <- initHMM(s, m)
			M <- $M \cup \{\lambda\}$
		endf

		for i in $|M|$
			for j in $|M|$
				$D_{ij}$ <- $1/2(L(S_i|M_j) + L(S_j|M_i))$
			endf
		endf

		P <- cluster(S, Z, k)
		$M'$ <- []

		for c in P
			$s'$ <- join(c)
			$\lambda$ <- initHMM($s'$, m)
			$M'$ <- $\lambda$::$M'$
		endf

		$\lambda'$ <- compositeHMM($M'$)
		$\lambda'$ <- BaumWelch($\lambda'$)
		return $\lambda'$
	end
	\end{lstlisting}
	\caption{Pseudocode for \citet{smyth}'s HMM training algorithm.}
	\label{fig:hmmcluster}
\end{figure}
To evaluate HMMCluster experimentally, \citeauthor{smyth} generated 20 real-valued, univariate time series of 200 observations from 2 HMMs with similar dynamics. He then used HMMCluster to train a 2 HMM model on the set of series. The HMMs yielded by \citeauthor{smyth}'s algorithm were very accurate - no transition probability was off by greater than .02 from that of the corresponding generator HMM, and $\pi$ and $B$ were both similarly close to their true values.

\section{Estimating $k$ With Monte-Carlo Cross-Validation}
\citeauthor{smyth} presents a method for estimating the optimal $k$ for HMMCluster via Monte-Carlo sampling. For each potential $k$, $X$ is randomly partitioned into a testing set $X_{test}$ of size $\beta$ and a training set $X_{train} = X \setminus X_{test}$. A model $\lambda'$ with $k$ clusters is then trained on $X_{train}$ with HMMCluster, and the log--likelihood $L(X_{test}|\lambda)$ recorded. This process is repeated over $n$ trials, and at the end the mean log--likelihood is computed for each $k$. By plotting a bar chart of $k$ vs mean log-likelihood, one can then see which $k$ achieved the highest score. In the ideal case, a large peak occurs at $X$'s true $k$ value.
