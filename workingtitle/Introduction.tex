\chapter{Introduction}

\section{Complexity Analysis}

The efficiency of programs is categorized by how the resource usage of a
program increases with the input size in the limit.  This is often called the
asymptotic efficiency or complexity of a program.  Asymptotic efficiency
abstracts away the details of efficiency, allowing programs to be compared
without knowledge of specific hardware architecture or the size and shape of
the programs input (\citet{Cormen2001}).  However, traditional complexity
analysis is first-order; the asymptotic efficiency of a program is only
expressed in terms of its input.  Consider the following function.
%
\lstset{language=[Objective]Caml}
\begin{lstlisting}
let rec map f xs =
  match xs with
  | [] -> []
  | (x::xs') -> f x :: map f xs'
\end{lstlisting}
%
The function \T{map} applies a function to every element in a list.
Traditional analysis assumes the cost of applying its first argument is
constant.


Traditional complexity analysis proceeds as follows.  First we write a
recurrence for the cost.  \[ T(n) = c + T(n-1) \] The variable $n$ is the
length of the list and the constant \T{c} is the cost of applying the function
$f$ to an element of the list and then applying the cons function \T{::}. The
result is the asymptotic efficiency of \T{map} is linear in the length of the
list.

The are two problems with this approach. The first is that there is no formal
connection between the implementation of \T{map} and the recurrence $T(n)$. The
consequence is extraction of the correct recurrence relies on the absence of
human error, which the author of this thesis can attest the difficulty of
doing. A formalization of the connection between the source program and
the recurrence would prevent this. The translation from the source program to
the recurrence could be done using application of a series of rules and the
translation could even be automated.

The second is that the analysis assumes the cost of applying the function \T{f} to
each element in the list has a constant cost. If the cost function is has a
constant cost, such as fixed width integer addition, then this first-order
analysis is sufficient.  The cost of mapping a constant cost function over a
list will increase linearly in the size of the list.  However, first-order
complexity analysis will not accurately describe the cost of mapping a
nontrivial function over a list. The cost of mapping a quadratic time function
such as insertion sort over a list of lists depends not only on the length of
the list, but also on the length of the sublists.  A more accurate prediction
of the cost of this function can be obtained by taking into account the cost of
insertion sort.

For an example such \T{map}, it is simple enough to change our analysis to
reflect that applying the functions \T{f} does not have constant cost $c$, but
instead has cost $f_c(x)$, where $x$ is some notion of size of the elements of
the list. If the elements of the list are fixed width integers, then all $x$
would be equivalent, and $f_c(x)$ would be constant. If the elements of the
list are strings or lists than the notion of size could be their
length. If we only interpret the size of lists to be their lengths,
because then we have no information about the size of the elements we apply $f$
to. So we interpret lists as a pair of their largest element and their length. The
recurrence for the cost of \T{map} becomes $T(n,x) = (f_c(x) + c)n$. Our
analysis of the cost of \T{map} is now parameterized by the cost applying \T{f}
to the elements of the list. However, this does not allow us to analyze the
composition of functions. For example to analyze the cost of $g \circ f$, we
need to have a notion of the size of the result of $f$, as well as the cost. We
need to have a notion of the size of the result of $f$ in order to analyze the
cost of applying $g$ to the result of applying $f$ to some value.

The method of extracting recurrences presented in \citet{Danner2013} and
\citet{Danner2015} addresses these problems. The first problem with traditional
analysis is there is no formal connection between a program and the recurrence
for its cost. There is a language which progams are written in, which we will
refer to as the source language, and a language for recurrences, which we will
refer to as the complexity language. In traditional complexity analysis, the
source language is some programming language and the complexity language is
mathematics, and there is no formalization of the translation from the source
language to the complexity language.  There is also no proof that the
mathematical recurrence for the cost of the source language program is an upper
bound on the cost of executing the source language program.  \citet{Danner2013}
address this problem by formally defining a translation from a source language
to a complexity language and proving the recurrence in the complexity language
is an upper bound on the cost of executing the source language program. The
complexity language is also higher order. So if the source language program is
higher order, such as \T{map}, the recurrence in the complexity language will
reflect the cost of applying the higher order function. Finally, functions in
the complexity language are from potentials to complexitys, so the result of
applying a function results in both a cost of the function and a potential
which can be used to analyze the cost of applying another function to the
result.

\section{Previous Work}

\paragraph{}
\citet{Danner2007}, building on the work of others, introduced the
idea that the complexity of an expression consists of a cost, representing an
upper bound on the time it takes to evaluate the expression, and a potential,
representing the cost of future uses of the expression.  The notion of a
potential allows the analysis of higher-order expressions.  The complexity of a
higher-order function such as \T{map} depends on the potential of its argument
function.  They developed a type system for ATR, a call-by-value version of
System T, that consists of a part restricting the sizes of values of
expressions and a part restricting the cost of evaluating a expression.
Programs written in ATR are constrained by the type system to run in less
than type-2 polynomial time.  \citet{Danner2009} extended this work to express
more forms of recursion, in particular those required by insertion sort and
selection sort.

\paragraph{}
\citet{Danner2013} utilized the notion of thinking of the complexity of an
expression as a pair of a cost and a potential to statically analyze the
complexity of a higher-order functional language with structural list
recursion.  The expressions in the higher-order functional language with
structural list recursion, referred to as the source language, are mapped to
expressions in a complexity language by a translation function.  The translated
expression describes an upper bound on the complexity of the original programs.

\paragraph{}
\citet{Danner2015} built on this work to formalize the extraction
of recurrences from a higher-order functional language with structural
recursion on arbitrary inductive data types. Programs are written in a higher
order functional language, referred to as the source language. The programs are
translated into a complexity language, which is essentially for recurrences.
The result of the translation of an expression is a pair of a cost and a
potential. The cost is a bound on the steps required to evaluate the expression
to a value, the potential is a size which represents cost of future use of the
value. A bounding relation is used to prove the translation and denotational
semantics of the complexity language give an upper bound on the operational
cost of running the source program. The paper also presents a syntactic
bounding theorem, where the abstraction of values to sizes done syntactically
instead of semantically.  Arbitrary inductive data types are handled
semantically using programmer specified sizes of data types. Sizes must be
programmer specified because the structure of a data type does not always
determine the interpretation of the size of a data type. There also exist
different reasonable interpretations of size, and some may be preferable to
others depending on what is being analyzed.


\section{Contribution}

This thesis comes in three parts.
\paragraph{}
The first chapter contains a catalog of examples of the extraction of recurrences
from functional programs using the approach given by \citet{Danner2015}. These
examples illustrate how to apply the method to nontrivial examples. They also
serve to demonstrate common techniques for solving the extracted recurrences.
The examples also allow the comparison with other automated complexity methods
such as those given by \citet{Avanzini2015} and \citet{HoffHof2010},
highlighting their respective strengths and weaknesses. The examples include
reversing a list in quadratic, reversing a list in linear time, insertion sort,
parametric insertion sort, list map, and tree map.


\paragraph{}
The second chapter extends the analysis to parallel programs. We change costs from
from natural numbers to the cost graphs described in \citet{Harper2012PFPL}. A cost
graph represents the dependencies between subcomputations in a program. The
nodes of the graph are subcomputations of the program and an edge between two
nodes indicates the result of one computation is an input to the other. The
cost graph can be used to determine an optimal strategy for scheduling the
computation on multiple processors. The cost graph has two properties that we
are interested in, work and span. The work is the total steps required to run
the program, which corresponds to the steps a single processor must execute to
run the program. The span is the critical path; the longest number of steps
from the start to the end of the cost graph.

\paragraph{}
The third chapter demonstrates the recurrence for the potential does not depend on
the recurrence for the cost. Consequently, we can extract the recurrence for the
potential and analyze it independently. This is useful because it is often
easier to solve the cost and potential recurrences independently than it is to
solve the initial recurrence. We are also sometimes only interested in just the
potential or just the cost of a recurrence.
