\chapter{Introduction}

\section{Complexity Analysis}

The efficiency of programs is categorized by how the resource usage of a
program increases with the input size in the limit.  This is often called the
asymptotic efficiency or complexity of a program.  Asymptotic efficiency
abstracts away the details of efficiency, allowing programs to be compared
without knowledge of specific hardware architecture or the size and shape of
the programs input (\citet{Cormen2001}).  However, traditional complexity
analysis is first-order; the asymptotic efficiency of a program is only
expressed in terms of its input.  Consider the following function.
%
\lstset{language=[Objective]Caml}
\begin{lstlisting}
let rec map f xs =
  match xs with
  | [] -> []
  | (x::xs') -> f x :: map f xs'
\end{lstlisting}
%
The function \T{map} applies a function to every element in a list.
Traditional analysis assumes the cost of applying its first argument is
constant.




Traditional complexity analysis proceeds as follows.  First we write a
recurrence for the cost.  \[ T(n) = c + T(n-1) \] The variable $n$ is the
length of the list and the constant \T{c} is the cost of applying the function
$f$ to an element of the list and then applying the cons function \T{::}. The
result is the asymptotic efficiency of \T{map} is linear in the length of the
list.



The are two problems with this approach.  The first is that the analysis
assumes the cost of applying the function \T{f} to each element in the list has
a constant cost. If the cost function has a constant cost, such as fixed
width integer addition, then this first-order analysis is sufficient.  The cost
of mapping a constant cost function over a list will increase linearly in the
size of the list.  However, first-order complexity analysis will not accurately
describe the cost of mapping a nontrivial function over a list. The cost of
mapping a quadratic time function such as insertion sort over a list of lists
depends not only on the length of the list, but also on the length of the
sublists.  A more accurate prediction of the cost of this function can be
obtained by taking into account the cost of the mapped function.



The second is that there is no formal connection between the implementation of
\T{map} and the recurrence $T(n)$. The consequence is extraction of the correct
recurrence relies on the absence of human error, to which the author of this
thesis can attest the difficulty of doing. A formalization of the connection
between the source program and the recurrence would prevent this. The
translation of the source program to the recurrence could be done by
application of a series of rules. A mechanical process such as this could
easily be automated.



For an example such \T{map}, it is simple enough to change our analysis to
reflect that applying the function \T{f} does not have constant cost $c$, but
instead has cost $f_c(x)$, where $x$ is some notion of size of the elements of
the list. If the elements of the list are fixed width integers, then all $x$
are equivalent, and $f_c(x)$ is constant. However, if the elements of the list
are strings or lists than $f_c(x)$ depends on the sizes of the elements of the
list.  If we only interpret the size of lists to be their lengths, we have no
information about the size of the elements we apply $f$ to. So we interpret
lists as a pair of their largest element and their length. The recurrence for
the cost of \T{map} becomes $T(n,i) = (f_c(i) + c)n$, where $i$ is the size of
the largest element of the list. Our analysis of the cost of \T{map} is now
parameterized by the cost applying \T{f} to the elements of the list. However,
this does not allow us to analyze the composition of functions. For example, to
analyze the cost of $g \circ f$, we need to have a notion of the size of the
result of $f$, as well as the cost. We need to have a notion of the size of the
result of $f$ in order to analyze the cost of applying $g$ to the result of
applying $f$ to some value.


The term we will use for this notion of the size is potential.  Potential
represents the cost of future use of an expression. As mentioned above,
potential is necessary to compose the analysis of functions. Consider this
implementation of \T{fromList} which creates a set from a list of items.
%
\begin{lstlisting}
fromList xs = foldr insert empty xs
\end{lstlisting}
%
The \T{insert} function takes an element and a set and adds the element to the
set. \T{empty} is the empty set. The \T{insert} function is applied to
increasing sized sets each step of the fold. To correctly analyze \T{fromList},
our analysis of \T{insert} must include both a cost of inserting an element
into a set, and a potential with which we can use to analyze the cost of the
next application of \T{insert} by \T{foldr}.



\section{Previous Work}

As we have just seen, traditional complexity analysis does not have a formal
connection between the programs and the extracted recurrences. Traditional
complexity analysis is also not compositional.



\citet{Danner2007}, building on the work of others, introduced the
idea that the complexity of an expression consists of a cost, representing an
upper bound on the time it takes to evaluate the expression, and a potential,
representing the cost of future uses of the expression. They developed ATR, a
variant of System T with call-by-value semantics and type system which
restricted the evaluation time of programs. ATR programs are limited to second
order programs, which take natural numbers and first-order programs as
arguments. Programs written in ATR are guaranteed to run in polynomial time. ATR
is at least powerful enough to compute each type-2 basic feasible functionals
characterized by \citet{KC96}. In order to limit the size of higher-order
programs, the type system of ATR limits both the size of the values of
expressions and the time required to evaluate an expression. A type-2 function
takes a function as an argument. In order to restrict the cost of evaluating
the type-2 function, we need to restrict the size of the output of the argument.
\citet{Danner2009} extended the ATR formalism with more forms of recursion, in
particular those required by insertion sort and selection sort.

%Consider the
%Ackermann function.
%%
%\[
%  A(n,m) =
%  \begin{cases}
%    n + 1              &\text{if } m=0 \\
%    A(m-1,1)           &\text{if } m>0 \text{ and } n=0 \\
%    A(m-1, A(m, n-1))  &\text{if } m>0 \text{ and } n>0
%  \end{cases}
%\]
%%
%The Ackerman function is definable in System T \citet{Harper2012PFPL}. Let
%\T{it:(nat$\to$nat)$\to$nat$\to$nat$\to$nat} be the function
%%
%type-2 function which composes its arguments.
%%
%\[
%  \lambda f.\lambda g.\lambda x. f\ (g\ x)
%\]
%%
%In order to restrict the cost of this function we need to restrict the size of
%the result of $g\ x$. This is why the type system restricts both the cost the
%evaluate an expression to a value and the size of the value of an expression.



Instead of implicitly restricting the complexity of programs as in ATR, the work of
\citet{Danner2013} focused on constructing recurrences that bound the
complexity of a program. The programs are written in a version of System T with
structural list recursion, referred to as the source language.  Programs in the
source language are limited to integers and integer lists, with structural
recursion on lists the only recursion construct.  A translation function maps
source language programs to recurrences in a complexity language.  It is not
possible for the user to define their own datatypes. The result of the
translation of a source language program is a complexity. The complexity
consists of a cost and a potential. The cost is a bound on the execution cost
of the program and the potential is the size of the result of evaluating the
program. To understand why the complexity must have both a cost and a
potential, consider the higher-order program \T{foldr} over a list.
%
\begin{lstlisting}
foldr f z xs =
  case xs of
    [] -> z
    x:xs' -> f x (fold f z xs')
\end{lstlisting}
%
To analyze the cost of applying \T{f} at each step of the fold, we must have a
bound on the size of \T{x} and \T{fold f z xs'}. In other words, the analysis
must produce a bound on the cost of the recursive call and a bound on the
size of the recursive call.

Costs and potentials also enable compositional analysis. Consider the
composition of two functions, \T{map sort} and \T{permutations}.
%
\begin{lstlisting}
map sort $\circ$ permutations
\end{lstlisting}
%
To analyze the cost of \T{map sort}, we must have a bound on its input size.
The size of the input to \T{map sort} is the size of the output of
\T{permutations}. So our analysis of permutations must produce a cost bound and
a size bound, which we can use to produce a cost bound for \T{map sort}.



\citet{Danner2015} built on this work to formalize the extraction
of recurrences from a higher-order functional language with structural
recursion on arbitrary inductive data types. Programs are written in a higher
order functional language, referred to as the source language. The programs are
translated into a complexity language, which is essentially a language for
recurrences.  The result of the translation of an expression is a pair of a
cost and a potential. The cost is a bound on the steps required to evaluate the
expression to a value, the potential is a size which represents cost of future
use of the value. A bounding relation is used to prove the translation and
denotational semantics of the complexity language give an upper bound on the
operational cost of running the source program. The paper also presents a
syntactic bounding theorem, where the abstraction of values to sizes done
syntactically instead of semantically.  Arbitrary inductive data types are
handled semantically using programmer specified sizes of data types. Sizes must
be programmer specified because the structure of a data type does not always
determine the interpretation of the size of a data type. There also exist
different reasonable interpretations of size, and some may be preferable to
others depending on what is being analyzed.


\section{Contribution}

This thesis comes in three parts.



Chapter 2 contains a catalog of examples of the extraction of recurrences
from functional programs using the approach given by \citet{Danner2015}. These
examples illustrate how to apply the method to nontrivial programs. They also
serve to demonstrate common techniques for solving the extracted recurrences.
The examples include reversing a list in quadratic time, reversing a list in
linear time, insertion sort, parametric insertion sort, list map, and tree map.
Linear time list reversal is an example of higher-order analysis. Slow list
reversal is an example of a quadratic time function. Parametric insertion sort
demonstrates the compositionality of the method as well as its ability to
handle higher-order programs.  We do list map and tree map to compare with the
parallel list map and tree map in Chapter 3.




Chapter 3 extends the analysis to parallel programs. The source language syntax
remains unchanged, but the operational semantics change to allow binary
fork-join parallelism, also called nested parallelism. The semantics are
parallel in that the subexpressions to tuples and function application may be
evaluated in parallel. Parallelism is nested because the subexpressions
themselves may have subexpressions which may also be evaluated in parallel. We
change costs from from natural numbers to the cost graphs described in
\citet{Harper2012PFPL}. A cost graph represents the dependencies between
subcomputations in a program.  The nodes of the graph are subcomputations of
the program and an edge between two nodes indicates the result of one
computation is an input to the other. The cost graph can be used to determine
an optimal strategy for scheduling the computation on multiple processors. The
cost graph has two properties that we are interested in, work and span. The
work is the total steps required to run the program, which corresponds to the
steps a single processor must execute to run the program.  The span is the
critical path; the longest number of steps from the start to the end of the
cost graph.



Chapter 4 defines a pure potential translation. The pure potential translation
is a stripped down version of the complexity translation which drops all
notions of cost. We prove by logical relations that for all well-typed source
language terms, the potential of the translation of the program into the
complexity language is related to the pure potential translation of the
program. The result of this is that the potential of the complexity of the
translation does not depend on the cost. This justifies the extraction of the
potential recurrence from the complexity language recurrence. This is useful
because it is often easier to solve the cost and potential recurrences
independently than it is to solve the initial recurrence. We are also sometimes
only interested in just the potential or just the cost of a recurrence.
