\chapter{Introduction}

\section{Complexity Analysis}

The efficiency of programs is categorized by how the resource usage of a
program increases with the input size in the limit.  This is often called the
asymptotic efficiency or complexity of a program.  Asymptotic efficiency
abstracts away the details of efficiency, allowing programs to be compared
without knowledge of specific hardware architecture or the size and shape of
the programs input (\citet{Cormen2001}).  However, traditional complexity
analysis is first-order; the asymptotic efficiency of a program is only
expressed in terms of its input.  Consider the following function.
%
\lstset{language=[Objective]Caml}
\begin{lstlisting}
let rec map f xs =
  match xs with
  | [] -> []
  | (x::xs') -> f x :: map f xs'
\end{lstlisting}
%
The function \T{map} applies a function to every element in a list.
Traditional analysis assumes the cost of applying its first argument is
constant.




Traditional complexity analysis proceeds as follows.  First we write a
recurrence for the cost.  \[ T(n) = c + T(n-1) \] The variable $n$ is the
length of the list and the constant \T{c} is the cost of applying the function
$f$ to an element of the list and then applying the cons function \T{::}. The
result is the asymptotic efficiency of \T{map} is linear in the length of the
list.



The are two problems with this approach.  The first is that the analysis
assumes the cost of applying the function \T{f} to each element in the list has
a constant cost. If the cost function is has a constant cost, such as fixed
width integer addition, then this first-order analysis is sufficient.  The cost
of mapping a constant cost function over a list will increase linearly in the
size of the list.  However, first-order complexity analysis will not accurately
describe the cost of mapping a nontrivial function over a list. The cost of
mapping a quadratic time function such as insertion sort over a list of lists
depends not only on the length of the list, but also on the length of the
sublists.  A more accurate prediction of the cost of this function can be
obtained by taking into account the cost of insertion sort.



The second is that there is no formal connection between the implementation of
\T{map} and the recurrence $T(n)$. The consequence is extraction of the correct
recurrence relies on the absence of human error, which the author of this
thesis can attest the difficulty of doing. A formalization of the connection
between the source program and the recurrence would prevent this. The
translation from the source program to the recurrence could be done using
application of a series of rules and the translation could even be automated.




For an example such \T{map}, it is simple enough to change our analysis to
reflect that applying the functions \T{f} does not have constant cost $c$, but
instead has cost $f_c(x)$, where $x$ is some notion of size of the elements of
the list. If the elements of the list are fixed width integers, then all $x$
would be equivalent, and $f_c(x)$ would be constant. If the elements of the
list are strings or lists than the notion of size could be their
length. If we only interpret the size of lists to be their lengths,
because then we have no information about the size of the elements we apply $f$
to. So we interpret lists as a pair of their largest element and their length. The
recurrence for the cost of \T{map} becomes $T(n,x) = (f_c(x) + c)n$. Our
analysis of the cost of \T{map} is now parameterized by the cost applying \T{f}
to the elements of the list. However, this does not allow us to analyze the
composition of functions. For example to analyze the cost of $g \circ f$, we
need to have a notion of the size of the result of $f$, as well as the cost. We
need to have a notion of the size of the result of $f$ in order to analyze the
cost of applying $g$ to the result of applying $f$ to some value.


The term we will this notion of the size of the result is potential. Potential
represents the cost of future use of an expression. As mentioned above,
potential is necessary to compose the analysis of functions. Consider this
implementation of \T{fromList} which creates a set from a list of items.
%
\lstset{language=Haskell}
\begin{lstlisting}
fromList xs = foldr `insert` empty xs
\end{lstlisting}
%
The \T{insert} function takes an element and a set and adds the element to the
set. \T{empty} is the empty set. The \T{insert} function will be with
increasing sized sets each step of the fold. To correctly analyze \T{fromList},
our analysis of \T{insert} must include both a cost of inserting an element
into a set, and a potential with which we can use to analyze the cost of the
next application of \T{insert} by \T{foldr}.


The denotational interpretation of the complexity language is standard. We
interpret numbers as elements of $\mathbb{Z}$, tuples of type $\tau \times
\sigma$ as elements of the cross-product of the set of values of type $\tau$
and the set of values of type $\sigma$, lambda expressions of type $\tau \to
\sigma$ as mathematical functions from the set of values of type $\tau$ to type
$\sigma$, and application as mathematical function application. The nonstandard
intepretations are those of constructors and \T{rec}. Since datatypes are
user-defined and there are multiple interpretations for a single datatype, the
user must provide their own translation. In this example we will interpret
\T{list} as the length.
%
\[
  \LB \T{list} \RB = \mathbb{N}
\]
%
Semantically, we will need to be able to distinguish between the constructors
of a datatype, so we also define a semantic value $D^\T{list}$. $D^\T{list}$ is
a sum type of the arguments to the \T{list} constructors. \T{list} has two
constructors, \T{Nil}, which has argument of type \T{unit}, and \T{Cons}, which
has argument of type $\T{int} \times \T{list}$.
%
\[
  D^\T{list} = \ast + \mathbb{Z} \times \mathbb{N}
\]
%
We will write $C_i$ for the $i^{th}$ injection. $C_i$ takes us from the
interpretation of the argument of a constructor to a value of type
$D^\T{list}$, and we also need a function $size_{list}$ which takes us from
$D^\T{list}$ back to $\llbracket \T{list} \rrbracket$. $size_\Delta$ is the
programmers notion of size for user-defined datatypes. In this case, we want
the size of a list to be its length. So our $size$ function is defined as follows.
%
\begin{align*}
  size_{list} (\ast) &= 0 \\
  size_{list}(i,n) &= 1 + n
\end{align*}
%
There is a restriction on the definition of the $size$ function. The size of a
value must be strictly greater than the size of any of its substructures of the
same type. In this case it means the size of a $(1, n)$ must be strictly
greater than the size of $(j, n-1)$.

The interpretation of a datatype with constructor $C$ under the environment
$\xi$ is
%
\[
  \llbracket C\ e\rrbracket \xi = size(C (\llbracket e \rrbracket \xi))
\]
%
In our case, if $e$ is \T{Nil}, then the interpretation $\llbracket \T{Nil}
\rrbracket$ is $size_{list}(C_0 \LB\LP\RP\RB)$, since the argument to the \T{Nil}
constructor is $\LP\RP$. The intepretation of \T{unit} is $0$. So
$size_{list}(C_0 \LB\LP\RP\RB = size_{list}(C_0\ 0)$. $C_0$ is the $0^{th}$
injection from $\LB \Phi[\T{list}\ \RB$ to $D^\T{list}$.  So
$size_{list}(C_0\ 0) = size_{list}(\ast)$. By our definition of
$size_{list}$, $size_{list}(\ast) = 0$.



The interpretation of \T{rec} is also nonstandard. To interpret \T{rec}, we
introduce a semantic $case$ function.
%
\[
  case^\delta : D^\delta \times \Uppi_{C} (S^{\LB \Phi_C[\delta]\RB} \to S^\tau) \to S^\tau
\]
%
$S^T = \LB T \RB$ for each complexity type $T$. The interpretation of a \T{rec} is
%
\[
  \LB rec^\delta(E^\delta, \overline{C \mapsto x^{\phi_C[\delta \times \tau]}.E_C^\tau}) \RB \xi = \bigvee\limits_{size(z) \leq \LB E \RB \xi} case(z, \overline{f_C})
\]
%
where for each constructur $C$,
%
\[
  f_C(x) = \LB E_C \RB \xi \{ x \mapsto map^{\Phi_C}(a.(a, \LB rec(w, \overline{C \mapsto x.E_C})\RB \xi \{ w \mapsto a \}), x) \}
\]
%
Since we cannot predict which branch the \T{rec} will take, we must take the
maximum over all possible branches to obtain an upper bound. Recall our
restriction on the $size$ function that the size of a value must be strictly
greater than the size of any of its substructure of the same type. This ensures
the recursion used to interpret the \T{rec} expressions is well-defined.


\section{Previous Work}

As we have just seen, traditional complexity analysis does not have a formal
connection between the programs and the extracted recurrences. Traditional
complexity analysis is also not compositional.

%The method of extracting recurrences presented in \citet{Danner2013} and
%\citet{Danner2015} addresses these problems. The first problem with traditional
%analysis is there is no formal connection between a program and the recurrence
%for its cost. There is a language which programs are written in, which we will
%refer to as the source language, and a language for recurrences, which we will
%refer to as the complexity language. In traditional complexity analysis, the
%source language is some programming language and the complexity language is
%mathematics, and there is no formalization of the translation from the source
%language to the complexity language.  There is also no proof that the
%mathematical recurrence for the cost of the source language program is an upper
%bound on the cost of executing the source language program.  \citet{Danner2013}
%address this problem by formally defining a translation from a source language
%to a complexity language and proving the recurrence in the complexity language
%is an upper bound on the cost of executing the source language program. The
%complexity language is also higher order. So if the source language program is
%higher order, such as \T{map}, the recurrence in the complexity language will
%reflect the cost of applying the higher order function. Finally, functions in
%the complexity language are from potentials to complexities, so the result of
%applying a function results in both a cost of the function and a potential
%which can be used to analyze the cost of applying another function to the
%result.


\citet{Danner2007}, building on the work of others, introduced the
idea that the complexity of an expression consists of a cost, representing an
upper bound on the time it takes to evaluate the expression, and a potential,
representing the cost of future uses of the expression.  The notion of a
potential allows the analysis of higher-order expressions.  The complexity of a
higher-order function such as \T{map} depends on the potential of its argument
function.  They developed a type system for ATR, a call-by-value version of
System T, that consists of a part restricting the sizes of values of
expressions and a part restricting the cost of evaluating a expression.
Programs written in ATR are constrained by the type system to run in less
than type-2 polynomial time.  \citet{Danner2009} extended this work to express
more forms of recursion, in particular those required by insertion sort and
selection sort.



\citet{Danner2013} utilized the notion of thinking of the complexity of an
expression as a pair of a cost and a potential to statically analyze the
complexity of a higher-order functional language with structural list
recursion.  The expressions in the higher-order functional language with
structural list recursion, referred to as the source language, are mapped to
expressions in a complexity language by a translation function.  The translated
expression describes an upper bound on the complexity of the original programs.



\citet{Danner2015} built on this work to formalize the extraction
of recurrences from a higher-order functional language with structural
recursion on arbitrary inductive data types. Programs are written in a higher
order functional language, referred to as the source language. The programs are
translated into a complexity language, which is essentially for recurrences.
The result of the translation of an expression is a pair of a cost and a
potential. The cost is a bound on the steps required to evaluate the expression
to a value, the potential is a size which represents cost of future use of the
value. A bounding relation is used to prove the translation and denotational
semantics of the complexity language give an upper bound on the operational
cost of running the source program. The paper also presents a syntactic
bounding theorem, where the abstraction of values to sizes done syntactically
instead of semantically.  Arbitrary inductive data types are handled
semantically using programmer specified sizes of data types. Sizes must be
programmer specified because the structure of a data type does not always
determine the interpretation of the size of a data type. There also exist
different reasonable interpretations of size, and some may be preferable to
others depending on what is being analyzed.


\section{Contribution}

This thesis comes in three parts.



Chapter 2 contains a catalog of examples of the extraction of recurrences
from functional programs using the approach given by \citet{Danner2015}. These
examples illustrate how to apply the method to nontrivial examples. They also
serve to demonstrate common techniques for solving the extracted recurrences.
The examples include reversing a list in quadratic, reversing a list in linear
time, insertion sort, parametric insertion sort, list map, and tree map. We do
quadratic time reverse as an introductory example. Linear time list reversal is
an example of higher-order analysis. Insertion sort introduces the
compositionality of analysis. Parametric insertion sort combines the
compositionality of insertion sort with a higher-order function. We do list map
and tree map to compare with the parallel list map and tree map.




Chapter 3 extends the analysis to parallel programs. The source language syntax
remains unchanged, but the operational semantics change to allow binary
fork-join parallelism, also called nested parallelism. The subexpressions to
tuples and function application may be evaluated in parallel. The
subexpressions themselves may have subexpressions which may also be evaluated
in parallel. We change costs from from natural numbers to the cost graphs
described in \citet{Harper2012PFPL}. A cost graph represents the dependencies
between subcomputations in a program.  The nodes of the graph are
subcomputations of the program and an edge between two nodes indicates the
result of one computation is an input to the other. The cost graph can be used
to determine an optimal strategy for scheduling the computation on multiple
processors. The cost graph has two properties that we are interested in, work
and span. The work is the total steps required to run the program, which
corresponds to the steps a single processor must execute to run the program.
The span is the critical path; the longest number of steps from the start to
the end of the cost graph.



Chapter 4 defines a pure potential translation. The pure potential translation
is a stripped down version of the complexity translation which drops all
notions of cost. We prove by logical relations that for all well-typed source
language terms, the potential of the translation of the program into the
complexity language is related to the pure potential translation of the
program. The result of this is that the potential of the complexity of the
translation does not depend on the cost. This justifies the extraction of the
potential recurrence from the complexity language recurrence. This is useful
because it is often easier to solve the cost and potential recurrences
independently than it is to solve the initial recurrence. We are also sometimes
only interested in just the potential or just the cost of a recurrence.
