\chapter{Parallel Functional Program Analysis}

We demonstrate the flexibility of the method developed in \citet{Danner2015} by
extending it to parallel cost semantics. We introduce a parallel operational
cost semantics for the source language, alter the translation into the
complexity language to produce a new notion of cost, and then prove the
bounding theorem for the new complexity translation function.  Finally we give
two examples of the recurrence extraction and interpretation.


To analyze the complexity of a sequential program, we are only interested in a
measure of the steps required to run the program. To analyze the complexity of
a parallel program, we need a measure which takes into account the extent to
which a computation may be run on multiple processors.  The cost semantics we
use are work and span \citet{Harper2012PFPL}.  We give a brief overview of work
and span.


\section{Work and span}
Work and span is a method of predicting the running time of programs that may
be run on arbitrary number of processors. Instead of producing an approximation
of the number of steps required to execute a program, work and span instead
produces a cost graph; a specification of the dependencies between
subcomputations of the program. The cost graph can be compiled into two
measures, work and span. The work of a program corresponds to the total number
of steps required to execute the program.  The span is the number of steps
in the critical path.  The critical path is the longest number of steps that
must be executed sequentially.  The length of the critical path determines the
extent to which a program may be parallelized.  If the span is equal to the
work, than every step in the computation depends on the previous step, and the
subcomputations of the program cannot be run independently, so the program
cannot be parallelized. If the span is smaller than the work, then there are
subcomputations which may be run independently and the program may be
parallelized. The upper bound on the running time of a program is summarized by
theorem \ref{thm:ws_brents_theorem}.

\begin{theorem}[Brent's Theorem]
  \label{thm:ws_brents_theorem}
  A program with work $w$ and span $s$ may be evaluated on $p$ processors in $O(max(w/p,s))$ steps.
\end{theorem}

A cost graph is defined as follows.

\[ \mathcal{C} ::= 0\ |\  1\ |\  \mathcal{C} \oplus \mathcal{C}\ |\  \mathcal{C} \otimes \mathcal{C} \]

The operator $\oplus$ connects to cost graphs who must be combined
sequentially.  The operator $\otimes$ connects cost graphs which may be
combined in parallel.

The work of a cost graph is defined as
%
\begin{equation*}
  work(c) = \begin{cases}
    0 &\text{if } c = 0 \\
    1 &\text{if } c = 1 \\
    work(c_0) + work(c_1) &\text{if } c = c_0 \otimes c_1 \\
    work(c_0) + work(c_1) &\text{if } c = c_0 \oplus c_1
  \end{cases}
\end{equation*}
%
Since the work of a program is the total number of steps required to run the
program, we add the work of subgraphs regardless whether the may be run
independently or not.


The span of a cost graph is defined as
%
\begin{equation*}
  span(c) = \begin{cases}
    0 &\text{if } c = 0 \\
    1 &\text{if } c = 1 \\
    max(span(c_0), span(c_1)) &\text{if } c = c_0 \otimes c_1 \\
    span(c_0) + span(c_1) &\text{if } c = c_0 \oplus c_1
  \end{cases}
\end{equation*}
%
Cost graphs connected by $\oplus$ must be run sequentially, so their span is the
sum of the spans of the subgraphs. Cost graphs connected by $\otimes$ may be
run independently, so their span is the maximum of the spans of the subgraphs.


\subsection{Operational Cost Semantics}
%
We alter the operational cost semantics of the source language to produce a cost
graph instead of a natural number. Figure \ref{fig:ws_srclang_oper_sem} shows
the new operational cost semantics. For tuples, the subexpressions may be evaluated
in parallel, so the cost of evaluating a tuple is the cost graphs of the
subexpressions connected by $\otimes$.  For \T{split}, the second subexpression
depends on the result of the first subexpression, so the cost of evaluating the
\T{split} is the cost graphs of the subexpression connected with $\oplus$. In
every rule except for tuples and function application, we replace $+$ with
$\oplus$. Because tuples and function application are the two syntactic
forms which consist of multiple subexpressions whose evaluation does not depend
on each other.
%
\begin{figure}
\label{fig:ws_srclang_oper_sem}
\caption{Source language operational semantics}

\bigskip

\AxiomC{$e_0 \downarrow^{n_0} v_0$}
\AxiomC{$e_1 \downarrow^{n_1} v_1$}
\BinaryInfC{$\LP e_0, e_1 \RP \downarrow^{n_0 \otimes n_1} \LP v_0, v_1 \RP$}
\DisplayProof
%
\quad
%
\AxiomC{$e_0 \downarrow^{n_0} \LP v_0, v_1 \RP$}
\AxiomC{$e_1[v_0/x_0, v_1/x_1] \downarrow^{n_1} v$}
\BinaryInfC{$split(e_0, x_0.x_1.e_1) \downarrow^{n_0 \oplus n_1} v$}
\DisplayProof

\bigskip

\AxiomC{$e_0 \downarrow^{n_0} \lambda x.e_0'$}
\AxiomC{$e_1 \downarrow^{n_1} v_1$}
\AxiomC{$e_0'[v_1/x] \downarrow^n v$}
\TrinaryInfC{$e_0\ e_1 \downarrow^{(n_0 \otimes n_1) \oplus n \oplus 1} v$}
\DisplayProof
%
\quad
%
\AxiomC{}
\UnaryInfC{$delay(e) \downarrow^0 delay(e)$}
\DisplayProof

\bigskip

\AxiomC{$e \downarrow^{n_0} delay(e_0)$}
\AxiomC{$e_0 \downarrow^{n_1} v$}
\BinaryInfC{$force(e) \downarrow^{n_0 \oplus n_1} v$}
\DisplayProof
%
\quad
%
\AxiomC{$e \downarrow^n v$}
\UnaryInfC{$C e \downarrow^n C v$}
\DisplayProof

\bigskip

\AxiomC{$e \downarrow^{n_0} C v_0$}
\AxiomC{$map^{\phi_C}(y.\LP y, delay(rec(y, \overline{C \mapsto x.e_C}))\RP, v_0) \downarrow^{n_1} v_1$}
\AxiomC{$e_C[v_1/x] \downarrow^{n_2} v$}
\TrinaryInfC{$rec(e, \overline{C \mapsto x.e_C}) \downarrow^{1 \oplus n_0 \oplus n_1 \oplus n_2} v$}
\DisplayProof

\bigskip

\AxiomC{}
\UnaryInfC{$map^t(x.v, v_0) \downarrow^0 v[v_0/x]$}
\DisplayProof
%
\quad
%
\AxiomC{}
\UnaryInfC{$map^\tau(x.v, v_0) \downarrow^0 v_0$}
\DisplayProof

\bigskip

\AxiomC{$map^{\phi_0}(x.v, v_0) \downarrow^{n_0} v_0'$}
\AxiomC{$map^{\phi_1}(x.v, v_1) \downarrow^{n_1} v_1'$}
\BinaryInfC{$map^{\phi_0 \times \phi_1}(x.v, \LP v_0, v_1 \RP) \downarrow^{n_0 \otimes n_1} \LP v_0', v_1'\RP$}
\DisplayProof

\bigskip

\AxiomC{}
\UnaryInfC{$map^{\tau \to \phi}(x.v, \lambda y.e) \downarrow^0 \lambda y.let(e, z.map^\phi(x.v, z))$}
\DisplayProof
%
\quad
%
\AxiomC{$e_0 \downarrow^{n_0} v_0$}
\AxiomC{$e_1[v_0/x] \downarrow^{n_1} v$}
\BinaryInfC{$let(e_0, x.e_1) \downarrow^{n_0 \oplus n_1} v$}
\DisplayProof
\end{figure}
%
The complexity translation is given in Figure
\ref{fig:ws_complexity_translation}.  The operator $E_0 \oplus_c E_1$ is
syntactic sugar for $\LP E_0 \oplus E_{1c}, E_{1p} \RP$.  The
translation is similar to the original translation except we replace the use of
$+$ and $+_c$ with $\oplus$ and $\oplus_c$.  In the tuple case and function
application case, the subexpressions may be computed in parallel so the cost is
the costs of the subgraphs connected with $\otimes$.
%
\begin{figure}
  \label{fig:ws_complexity_translation}
  \caption{Work and span translation from source language to complexity language}
  \begin{align*}
    \|x\| &= \LP 0, x \RP \\
    \|\LP\RP\| &= \LP 0, \LP \RP \RP \\
    \|\LP e_0, e_1 \RP \| &= \LP \|e_0\|_c \otimes \|e_1\|_c, \LP \|e_0\|_p, \|e_1\|_p\RP\RP \\
    \|split(e_0, x_0.x_1.e_1)\| &= \|e_0\|_c \oplus_c \|e_1\|[\pi_0\|e_0\|_p/x_0, \pi_1\|e_1\|_p/x_1] \\
    \|\lambda x.e\| &= \LP 0, \lambda x.\|e\| \RP \\
    \|e_0\ e_1\| &= 1 \oplus (\|e_0\|_c \otimes \|e_1\|_c) \oplus_c \|e_0\|_p\ \|e_1\|_p \\
    \|delay(e)\| &= \LP 0, \|e\|\RP \\
    \|force(e)\| &= \|e\|_c \oplus_c \|e\|_p \\
    \|C_i^\delta e\| &= \LP \|e\|_c, C_i^\delta \|e\|_p \RP \\
    \|rec^\delta(e, \overline{C \mapsto x.e_C})\| &= \|e\|_c \oplus_c rec^\delta(\|e\|_p, \overline{C \mapsto x.1 \oplus_c \|e_C\|}) \\
    \|map^\phi(x.v_0, v_1)\| &= \LP 0, map^{\LP\LP \phi \RP \RP} (x. \|v_0\|_p, \|v_1\|_p)\RP \\
    \|let(e_0, x.e_1)\| &= \|e_0\|_c \oplus_c \|e_1\|[\|e_0\|_p/x]
  \end{align*}
\end{figure}
%
\section{Bounding Relation}
%
We verify that the the translation of a well-typed source language is bounded
by its translation into the complexity language. We mutually define the
following bounding relations:
%
\begin{enumerate}
  \item $e \sqsubseteq_\tau E, \emptyset \vdash_\phi e : \tau$ and $\emptyset \vdash_{\|\psi\|} E : \|\tau\|$
  \item $v \sqsubseteq_\tau^{val} E, $ where $\emptyset \vdash v : \tau$ and $\emptyset \vdash_{\|\psi\|} E : \llangle \tau \rrangle$
  \item $v \sqsubseteq_{\phi,R}^{val} E, $ where $ \emptyset \vdash_\psi v : \phi[\gamma]$ and $\emptyset \vdash_{\|\psi\|} E : \llangle \phi \rrangle[\delta]$
  \item $e \sqsubseteq_{\phi,R} E$, where $\emptyset \vdash_\psi e : \phi[\delta]$ and $\emptyset \vdash_{\|\psi\|} E : \|\phi\|[\delta]$
\end{enumerate}
%
The bounding relation $e \sqsubseteq_\tau$ defines the notion of an source
language expression to be bounded by a complexity language expression. We will
refer to this as "bounding". The second bounding relation $e
\sqsubseteq_\tau^{val} E$ defines a notion of a complexity language potential
bounding a source language value. We will refer to this relation as "value
bounding". Relations three and four are parameterized by any relation $R$ and
interpret strictly positive functors as relation transformers. The mutual
definitions of the relations are given below.
%
\begin{defn}[Bounding Relation]\leavevmode
  \label{def:ws_bounding_relations}
\begin{enumerate}
  \item We define $e \sqsubseteq_\tau E$ as if $e \downarrow^n v$, then \label{ws:bounding_rel_defn}
    \begin{itemize}
      \item $n \leq E_c$
      \item $v \sqsubseteq_\tau^{val} E_p$.
    \end{itemize}
  \item We define $v \sqsubseteq_\tau^{val} E$ as
    \begin{itemize}
      \item $v \sqsubseteq_{unit}^{val} E$ always.
      \item $\LP v_0,v_1 \RP \sqsubseteq_{\tau_0 \times \tau_1}^{val} E$ iff $v_0 \sqsubseteq_{\tau_0}^{val} \pi_0 E$ and $v_1 \sqsubseteq_{\tau_2}^{val} \pi_1 E$.
      \item $\T{delay}(e) \sqsubseteq_{\T{susp }\tau}^{val}$ if $e \sqsubseteq_\tau E$.
      \item $v \sqsubseteq_\delta^{val} E$ is inductively defined by
        \begin{prooftree}
          \AxiomC{$\textbf{C} : \phi \to \delta \in \psi$}
          \AxiomC{$v \sqsubseteq_{\phi, -\sqsubseteq_\delta^{val}-}^{val} E'$}
          \AxiomC{$\textbf{C}\ E' \leq_\delta E$}
          \TrinaryInfC{$\textbf{C}\ v \sqsubseteq_\delta^{val} E$}
        \end{prooftree}
      \item $\lambda x.e \sqsubseteq^{val}_{\tau \to \phi,R} E$ if for all $v$ and $E_0$, if $v \sqsubseteq_\tau^{val} E_0$, then $e[v/x] \sqsubseteq_{\phi,R}(E\ E_0)$
    \end{itemize}
  \item We define $v \sqsubseteq_{\phi,R}^{val} E_p$ as
    \begin{itemize}
      \item $v \sqsubseteq_{t,R}^{val} E $ if $R(v,E)$
      \item $v \sqsubseteq_{\tau,R}^{val} E$ if $v \sqsubseteq_\tau^{val} E$.
      \item $\LP v_0,v_1\RP \sqsubseteq_{\phi_0\times\phi_1,R}^{val} E$ if $v_0 \sqsubseteq_{\phi_0,R}^{val} \pi_0 E$ and $v_1 \sqsubseteq_{\phi_0,R}^{val} \pi_1 E$
    \end{itemize}
  \item We define as $e \sqsubseteq_{\phi,R} E$ as if $e \downarrow^n v$, then
    \begin{itemize}
      \item $n \leq E_c$
      \item $v \sqsubseteq_{\phi,R}^{val} E_p$
    \end{itemize}
\end{enumerate}
\end{defn}
%
The definition \ref{ws:bounding_rel_defn} of $e \sqsubseteq_\tau E$ states that
if a source language expression $e$ steps to a value $v$ with cost $n$, then
the cost $n$ is less than or equal to the cost of the complexity language
expression $E$ and the value $v$ is value bounded by potential of the
complexity language expression $E$. In the sequential cost semantics, the cost
is a natural number and $\leq$ is the ordering $a \leq b$ if there exists a
natural number $c$ such that $a+c=b$. We need to impose a partial ordering on
cost graphs. We use the containment ordering. A cost graph $a$ is less than or
equal to a cost graph $b$ if $a$ is a subset of $b$. The subset relation is
transitive and antisymmetric, so the ordering is transitive and antisymmetric.
The ordering must satisfy these two properties in order for the proof of the
bounding relation to fall through.


The bounding theorem states well-typed source language expressions are bounded
by their translations into the complexity language.
%The following lemmas are required.
% %Weakening consists of two parts. The first states if a source language
%expression is bounded by a complexity language expression and the complexity
%language expression is less than or equal to another complexity language
%expression, then the source language expression is bounded by the second
%complexity language expression. The second states if a source language value is
%value bounded by a complexity language expression and the complexity language
%expression is less than or equal to another complexity language expression,
%then the source language value is value bounded by the second complexity
%language expression.
%%
%\begin{lemma}[Weakening]
%  \label{lem:ws_weakening}
%  \begin{align}
%    &e \sqsubseteq_\tau E \ and\ E \leq_{\|\tau\|} E' \implies e \sqsubseteq_\tau E' \\
%    &v \sqsubseteq_\tau^{val} E \ and\ E \leq_{\llangle \tau \rrangle} E' \implies  v \sqsubseteq_\tau^{val} E'
%  \end{align}
%\end{lemma}
%\begin{proof}
%  The proof is by induction on type $\tau$.
%\end{proof}
%%
%The compositionality
%
\begin{theorem}[Bounding Theorem]
  If $\gamma \vdash e : \tau, $ then $e \sqsubseteq_\tau \|e\|$.
\end{theorem}
\begin{proof}
  The proof proceeds by induction on the typing derivation and is identical to
  the proof of the bounding theorem \citet{Danner2015}. We will not reproduce
  it here.
\end{proof}

%
% PARALLEL LIST MAP
%
\section{Parallel List Map}
We will analyze the cost of the \T{map} function from section
\ref{sec:sequential_list_map}. We translate the source language program using
the new translation function and interpret the resulting complexity language
expression in a similar denotational semantics.

We use the same data type \T{list} and \T{map} definition.
%
\begin{equation*}
  \T{datatype list} = \T{Nil of Unit | Cons of int $\times$ list}
\end{equation*}
%
\begin{equation*}
  \T{map} = \lambda f. \lambda xs . \T{rec}(xs, \T{Nil} \mapsto \T{Nil}, \T{Cons} \mapsto \LP y \LP ys, y \RP\RP. \T{Cons}\LP f\ y, \T{force}(r)\RP)
\end{equation*}
%
The derivation of the complexity expression is given in Figure
\ref{fig:ws_map_complexity_translation}.
%
\begin{figure}
  \label{fig:ws_map_complexity_translation}
  \caption{Work and span complexity translation of \T{map f xs}.}
  \[\begin{split}
    &\|\T{map f xs}\| = \\
    &  (2 \oplus f_c \otimes xs_c) \oplus_c \T{rec}(xs_p, \T{Nil}\mapsto 1 \oplus_c \|\T{Nil}\|, \\
    &\quadten\qquad \T{Cons} \mapsto \LP y, \LP ys, r \RP \RP. 1 \oplus_c \|\T{Cons}\LP f\ y, \T{force}(r)\RP \|) \\
    %Nil branch
    &\quad 1 \oplus_c \|\T{Nil}\| = 1 \oplus_c \LP 0, Nil \RP = \LP 1, Nil \RP \\
    %Cons branch
    &\quad\|\T{Cons}\LP f\ y, \T{force}(r)\RP \| = \LP \|\LP f\ y, \T{force}(r)\RP\|_c, \T{Cons} \| \LP f\ y, \T{force}(r)\RP \|_p\RP \\
    &\qquad \|\LP f\ y, \T{force}(r)\RP\| = \LP \|f\ y\|_c \otimes \|\T{force}(r)\|_c, \LP \|f\ y\|_p, \|\T{force}(r)\|_p\RP\RP \\
    %f y
    &\quadthree \|f\ y\| = (1 \oplus \|f\|_c \otimes \|y\|_c) \oplus_c \|f\|_p \|y\|_p \\
    &\quadfour = (1 \oplus \LP 0, f_p \RP_c \otimes \LP 0, y \RP_c) \oplus_c \LP 0, f_p \RP_p \LP 0, y \RP_p \\
    &\quadfour = 1 \oplus_c f_p\ y \\
    &\quadfour = \LP 1 \oplus (f_p y)_c, (f_p\ y)_p \RP \\
    %force r
    &\quadthree \|\T{force}(r)\| = \|r\|_c \oplus_c \|r\|_p \\
    &\quadfour = \LP 0, r \RP_c \oplus_c \LP 0, r \RP_p \\
    &\quadfour = r \\
    %<...>
    &\qquad \|\LP f\ y, \T{force}(r)\RP\| = \LP (1 \oplus (f_p\ y)_c) \otimes r_c, \LP (f_p\ y)_p, r_p \RP \RP \\
    %Cons<..>
    &\quad\|\T{Cons}\LP f\ y, \T{force}(r)\RP \| = \LP (1 \oplus (f_p\ y)_c) \otimes r_c, \T{Cons} \LP (f_p\ y)_p, r_p \RP \RP \\
    %rec
    &\|\T{map f xs}\| = (2 \oplus f_c \otimes xs_c) \oplus_c \\
    &\quad \T{rec}(xs_p, \T{Nil}\mapsto \LP 1, \T{Nil}\RP, \\
    &\quadfour \T{Cons} \mapsto \LP y, \LP ys, r \RP \RP. \LP 1 \oplus ((1 \oplus (f_p\ y)_c) \otimes r_c), \T{Cons} \LP (f_p\ y)_p, r_p \RP \RP)
  \end{split}\]
\end{figure}
%
The complexity language translation is
%
\begin{equation*}
\begin{split}
    &\|\T{map f xs}\| = (2 \oplus f_c \otimes xs_c) \oplus_c \\
    &\quad \T{rec}(xs_p, \T{Nil}\mapsto \LP 1, \T{Nil}\RP, \\
    &\quadfour \T{Cons} \mapsto \LP y, \LP ys, r \RP \RP. \LP 1 \oplus ((1 \oplus (f_p\ y)_c) \otimes r_c), \T{Cons} \LP (f_p\ y)_p, r_p \RP \RP)
\end{split}
\end{equation*}
%
We interpret lists as a pair of their largest element and length.
%
\begin{align*}
  \LB \T{list} \RB &= \mathbb{Z} \times \mathbb{N} \\
  D^\T{list} &= \{*\} + (\LB \mathbb{Z} \RB \times \LB \T{list} \RB) \\
  size_\T{list}(*) &= (0, 0) \\
  size_\T{list}((x, (m, n))) &= (max(x, m), 1 + n)
\end{align*}
%
The interpretation of the recursor is given in Figure \ref{fig:ws_map_interpretation}.
%
\begin{figure}
  \label{fig:ws_map_interpretation}
  \caption{Interpretation of recursor in \T{map}}
  \[\begin{split}
      &\quad \text{Let } \eta = \{ xs \mapsto (0, (m, n)), f \mapsto (0, f)\}\\
      &\quad g(f, (m, n)) = \LB \T{rec}(xs_p, \T{Nil} \mapsto \LP 1, \T{Nil}\RP,\\
      &\quadten \T{Cons} \mapsto \LP y, \LP ys, r \RP \RP . \LP 1 \oplus (1 \oplus (f_p\ y)_c \otimes r_c), \T{Cons}\LP (f_p\ y)_p, r_p\RP\RP) \RB \eta\\
      &\qquad = \bigvee\limits_{(m_1, n_1) \leq (m, n)} case((m_1, n_1), \T{Nil} \mapsto \LB \LP 1, \T{Nil} \RP \RB \eta, \\
      &\quadten \T{N} \mapsto \LP y, \LP ys, r \RP \RP . \LB \LP 1 \oplus ((1 \oplus (f_p\ y)_c) \otimes r_c), \T{Cons} \LP (f_p\ y)_p, r_p \RP \RP \RB \eta_c) \\
      &\qquad \text{where } \eta_c = \{xs \mapsto (0, (m, n)), f \mapsto (0, f), y \mapsto m, ys \mapsto (0, (m, n)), \\
      &\quadeight r \mapsto g(f, (m, n-1)))\} \\
      %Nil branch
      &\quadthree \text{\T{Nil} branch} \\
      &\quadthree \LB \LP 1, \T{Nil} \RP \RB \eta = (\LB 1 \RB \eta , \LB \T{Nil} \RB \eta) = (1, (0, 0)) \\
      %Cons branch
      &\quadthree \text{\T{Cons} branch} \\
      &\quadthree  \LB \LP 1 \oplus ((1 \oplus (f_p\ m)_c) \otimes r_c), \T{Cons} \LP (f_p\ m)_p, r_p \RP \RP \RB \eta_c \\
      &\quadfour = (1 \oplus ((1 \oplus (f\ m)_c) \otimes g_c(f, (m, n-1))), ((f\ m)_p, \pi_1 g_p(f, (m, n-1)))) \\
      %putting branches together
      &\quad g(f, (m, n)) = \\
      &\qquad \bigvee\limits_{(m_1, n_1) \leq (m, n)} case((m_1, n_1), \T{Nil} \mapsto (1, (0, 0)), \\
      &\quadfour \T{Cons} \mapsto (1 \oplus ((1 \oplus (f\ m)_c) \otimes g_c(f, (m, n-1))), ((f\ m)_p, \pi_1 g_p(f, (m, n-1)))))
  \end{split}\]
\end{figure}
%
The result is
%
\begin{equation*}
  \begin{split}
  &g(f, (m, n)) = \\
  &\quad \bigvee\limits_{(m_1, n_1) \leq (m, n)} case((m_1, n_1), \T{Nil} \mapsto (1, (0, 0)), \\
  &\quadfour \T{Cons} \mapsto (1 \oplus ((1 \oplus (f\ y)_c) \otimes g_c(f, (m, n-1))), ((f\ y)_p, \pi_1 g_p(f, (m, n-1)))))
  \end{split}
\end{equation*}
%
%In figure \ref{fig:ws_map_massage}, we massage the recurrence into a simpler form by eliminating the big max and the $case$.
%\begin{figure}
%  \label{fig:ws_map_massage}
%  \caption{Simplification of the interpretation of \T{map}}
%  \[\begin{split}
%  &g(f, (m, n)) = \\
%  &\quad \bigvee\limits_{(m_1, n_1) \leq (m, n)} case((m_1, n_1), \T{Nil} \mapsto (1, (0, 0)), \\
%  &\quadfive \T{Cons} \mapsto (1 \oplus ((1 \oplus (f\ y)_c) \otimes g_c(f, (m, n-1))), ((f\ y)_p, \pi_1 g_p(f, (m, n-1)))))  \\
%      %base case
%  &\text{For } n = 0 \\
%  &\quad g(f, (m, 0) = \\
%  &\qquad (1, (0, 0)) \vee \bigvee\limits_{1 + n_1 } \\
%  &\quadsix \T{Cons} \mapsto (1 \oplus ((1 \oplus (f\ y)_c) \otimes g_c(f, (m, -1))), ((f\ y)_p, \pi_1 g_p(f, (m, -1)))))  \\
%  &\qquad = (1, (0, 0)) \\
%      %inductive case
%  &\text{For } n > 0 \\
%  &fubar
%  \end{split} \]
%\end{figure}
%
%The result is
%\begin{equation*}
%  g(f, (m, n)) = \begin{cases}
%    (1, (0, 0)) &n \equiv 0 \\
%    (1 \oplus (1 \oplus (f\ m)_c \otimes g(f, (m, n-1))_c), ((f\ m)_p, g(f, (m, n-1))_p)) &n > 0
%  \end{cases}
%\end{equation*}
We compile the recurrence down to the work and the span to make it easier to
manipulate.
%
\begin{equation*}
  \begin{split}
  &g(f, (m, n)) = \\
  &\quad \bigvee\limits_{(m_1, n_1) \leq (m, n)} case((m_1, n_1), \\
  &\quadfive\quadfour \T{Nil} \mapsto ((1, 1), (0, 0)), \\
  &\quadfive\quadfour \T{Cons} \mapsto ((2 + \pi_0(f\ y)_c + \pi_0 g_c(f, (m, n-1)), \\
  &\quadfive\quadten                     1 + max(1 + \pi_1(f\ y)_c, \pi_1 g_c(f, (m, n-1)))), \\
  &\quadfive\quadeight                  ((f\ y)_p, \pi_1 g_p(f, (m, n-1)))))
  \end{split}
\end{equation*}
%
We prove by induction bounds on the work and span of the cost of $g$.
%
\begin{theorem}
$\pi_0 g_c(f, (m, n)) \leq 1 + (2 + \pi_0(f\ m)_c)n$
\end{theorem}
%
\begin{proof}
   The proof is by induction on $n$.
  \begin{description}
    \item[case $n=0$]\mbox{}\\[-1.5\baselineskip]
      \begin{align*}
      \pi_0 g_c(f, (m, 0)) = \pi_0((1, 1), (0, 0))_c = \pi_0(1, 1) = 1
      \end{align*}
    \item[case $n>0$]\mbox{}\\[-1.5\baselineskip]
      \[\begin{split}
        &\pi_0(g_c(f, (m, n))) = \\
        &\quad \pi_0(\bigvee\limits_{(m_1, n_1) \leq (m, n)} case((m_1, n_1), \\
        &\quadfive\quadfour \T{Nil} \mapsto ((1, 1), (0, 0)), \\
        &\quadfive\quadfour \T{Cons} \mapsto ((2 + \pi_0(f\ m_1)_c + \pi_0g_c(f, (m_1, n_1-1)), \\
        &\quadfive\quadten                     1 + max(1 + \pi_1(f\ m_1)_c, \pi_1g_c(f, (m_1, n_1-1)))), \\
        &\quadfive\quadeight                  ((f\ m_1)_p, \pi_1 g_p(f, (m_1, n_1-1))))) )_c \\
        &\quad = 1 \vee \pi_0(\bigvee\limits_{(m_1, n_1) \leq (m, n)} ((2 + \pi_0(f\ m_1)_c + \pi_0g_c(f, (m_1, n_1-1)), \\
        &\quadfour\quadten                     1 + max(1 + \pi_1(f\ m_1)_c, \pi_1g_c(f, (m_1, n_1-1)))), \\
        &\quadfour\quadeight                  ((f\ m_1)_p, \pi_1 g_p(f, (m_1, n_1-1))))) )_c \\
        &\quad = 1 \vee \bigvee\limits_{(m_1, n_1) \leq (m, n)} 2 + \pi_0 (f\ m_1)_c + \pi_0 g_c(f, (m_1, n_1-1)) \\
        &\quad \leq \bigvee\limits_{(m_1, n_1) \leq (m, n)} 2 + \pi_0 (f\ m_1)_c + (1 + (2 + \pi_0 (f\ m_1)_c)(n_1-1)) \\
        &\quad \leq 2 + \pi_0 (f\ m)_c + 1 + (2 + \pi_0 (f\ m)_c)(n - 1) \\
        &\quad \leq 1 + (2 + \pi_0 (f\ m)_c)n \\
      \end{split}\]
  \end{description}
\end{proof}
%
\begin{theorem}
  $\pi_1 g_c(f, (m, n)) \leq 1 + \pi_1 (f\ m)_c + n$
\end{theorem}
%
\begin{proof}
  \begin{description}
    \item[case $n=0$]\mbox{}\\[-1.5\baselineskip]
      \begin{align*}
      \pi_1 g_c(f, (m, 0)) = \pi_1((1, 1), (0, 0))_c = \pi_1(1, 1) = 1
      \end{align*}
    \item[case $n>0$]\mbox{}\\[-1.5\baselineskip]
      \[\begin{split}
        &\pi_1(g_c(f, (m, n))) = \\
        &\quad \pi_1(\bigvee\limits_{(m_1, n_1) \leq (m, n)} case((m_1, n_1), \\
        &\quadfive\quadfour \T{Nil} \mapsto ((1, 1), (0, 0)), \\
        &\quadfive\quadfour \T{Cons} \mapsto ((2 + \pi_0(f\ m_1)_c + \pi_0g_c(f, (m_1, n_1-1)), \\
        &\quadfive\quadten                     1 + max(1 + \pi_1(f\ m_1)_c, \pi_1g_c(f, (m_1, n_1-1)))), \\
        &\quadfive\quadeight                  ((f\ m_1)_p, \pi_1 g_p(f, (m_1, n_1-1))))) )_c \\
        &\quad = 1 \vee \pi_1(\bigvee\limits_{(m_1, n_1) \leq (m, n)} ((2 + \pi_0(f\ m_1)_c + \pi_0g_c(f, (m_1, n_1-1)), \\
        &\quadfour\quadten                     1 + max(1 + \pi_1(f\ m_1)_c, \pi_1g_c(f, (m_1, n_1-1)))), \\
        &\quadfour\quadeight                  ((f\ m_1)_p, \pi_1 g_p(f, (m_1, n_1-1))))) )_c \\
        &\quad = 1 \vee \bigvee\limits_{(m_1, n_1) \leq (m, n)} 1 + max(1 + \pi_1 (f\ m_1)_c, \pi_1 g_c(f, (m_1, n_1-1))) \\
        &\quad \leq \bigvee\limits_{(m_1, n_1) \leq (m, n)} 1 + max(1 + \pi_1 (f\ m_1)_c, 1 + \pi_1 (f\ m_1)_c + n_1 - 1) \\
        &\quad \leq 1 + max(1 + \pi_1 (f\ m)_c, 1 + \pi_1 (f\ m)_c + n - 1) \\
        &\quad \leq 1 + 1 + \pi_1 (f\ m)_c + n - 1 \\
        &\quad \leq 1 + \pi_1 (f\ m)_c + n
      \end{split}\]
  \end{description}
\end{proof}
%
Compare these results with sequential map.


\section{Parallel Tree Map}
A program which is embarrasingly parallel is tree map.  When a function $f$ is
mapped over a tree $t$, each application of $f$ to the label at each node can
be done independently.  Furthermore, the tree data structure itself is
dividable by construction.  Dividing the work requires only destruction of the
node constructor to yield the left and right subtrees.

We will use \T{int} labelled binary trees.
%
\begin{equation*}
  \T{datatype tree} = \T{E of Unit | N of int$\times$tree$\times$tree}
\end{equation*}

%
\T{map} simply deconstructs each node, applies the function to the label,
recuses on the children, and reconstructs a node using the results.
%
\begin{equation*}
  \T{map} = \lambda f.\lambda t.\T{rec}(t, \T{E} \mapsto \T{E}, \T{N} \mapsto \LP x, \LP t_0, r_0 \RP, \LP t_1, r_1 \RP\RP.\T{N} \LP f\ x, \T{force}(r_0), \T{force}(r_1)\RP)
\end{equation*}
%
\begin{figure}
  \label{fig:ws_treemap_complexity_translation}
  \caption{Complexity translation of \T{map f t}}
  \begin{equation*}
    \begin{split}
      &2 \oplus (f_c \otimes t_c) \oplus_c \T{rec}(t_p, \T{E} \mapsto 1 \oplus_c \|\T{E}\|, \\
      &\quadten \T{N} \mapsto \LP y, \LP t_0, r_0 \RP \LP t_1, r_1 \RP \RP 1 \oplus_c \|\T{N}\LP f\ x, \T{force}(r_0) \T{force}(r_1)\RP \| ) \\
      %E branch
      &\quad 1 \oplus_c \|E\| = 1 \oplus \LP 0, E \RP = \LP 1, E \RP \\
      %N branch
      &\quad \|\T{N}\LP f\ x, \T{force}(r_0) \T{force}(r_1)\RP \| = \\
      &\qquad \LP \|\LP f\ x, \T{force}(r_0) \T{force}(r_1)\RP \|_c, \T{N} \|\LP f\ x, \T{force}(r_0) \T{force}(r_1)\RP\|_p\RP \\
      &\quadthree \|\LP f\ x, \T{force}(r_0) \T{force}(r_1)\RP \| = \\
      &\quadfour \LP \|f\ x\|_c \otimes \|\T{force}(r_0)\|_c \otimes \|\T{force}(r_1)\|_c, \LP \|f\ x\|_p, \|\T{force}(r_0)\|_p, \|\T{force}(r_1)\|_p \RP \RP \\
      % \|f x\|
      &\quadfive  \|f\ x\| = 1 \oplus \|f\|_c \otimes \|x\|_c \oplus_c \|f\|_p \|x\|_p \\
      &\quadeight = 1 \oplus \LP 0, f \RP_c \otimes \LP 0, x \RP_c \oplus_c \LP 0, f \RP_p \LP 0, x \RP_p \\
      &\quadeight = 1 \oplus_c (f_p\ x) \\
      &\quadeight = \LP 1 \oplus (f_p\ x)_c, (f_p\ x)_p \RP \\
      % \|force(r0)\|
      &\quadfive \|\T{force}(r_0)\| = \|r_0\|_c \oplus_c \|r_0\|_p \\
      &\quadeight = \LP 0, r_0 \RP \oplus_c \LP 0, r_0 \RP_p \\
      &\quadeight = \LP 0 + r_{0c}, r_{0p} \RP \\
      &\quadeight = r_0 \\
      % \|force(r1)\|
      &\quadfive \|\T{force}(r_1)\| = \|r_1\|_c \oplus_c \|r_1\|_p \\
      &\quadeight = \LP 0, r_1 \RP \oplus_c \LP 0, r_1 \RP_p \\
      &\quadeight = \LP 0 + r_{1c}, r_{1p} \RP \\
      &\quadeight = r_1  \\
      %\|<...>\|
      &\quadthree \|\LP f\ x, \T{force}(r_0) \T{force}(r_1)\RP \| = \\
      &\quadsix = \LP 1 \oplus (f_p\ x)_c \otimes r_{0c} \otimes r_{1c}, \LP (f_p\ x)_p, r_{0p}, r_{1p}\RP\RP \\
      %\|N<...>\|
      &\qquad \|\T{N}\LP f\ x, \T{force}(r_0) \T{force}(r_1)\RP \| = \\
      &\quadfive = \LP 1 \oplus (f_p\ x)_c \otimes r_{0c} \otimes r_{1c}, \T{N} \LP (f_p\ x)_p, r_{0p}, r_{1p}\RP\RP \\
      %rec
      &\|\T{map f t}\| = \\
      &2 \oplus (f_c \otimes t_c) \oplus 1 \oplus_c \T{rec}(t_p, \T{E} \mapsto \LP 1, \T{E}\RP, \\
      &\quadeight \T{N} \mapsto \LP y, \LP t_0, r_0 \RP \LP t_1, r_1 \RP \RP.  \LP 2 \oplus (f_p\ x)_c \otimes r_{0c} \otimes r_{1c}, \T{N} \LP (f_p\ x)_p, r_{0p}, r_{1p}\RP\RP
    \end{split}
  \end{equation*}
\end{figure}

The translation of \T{map f t} is given in figure \ref{fig:ws_treemap_complexity_translation}.
\begin{equation}
  \label{eq:ws_treemap_complexity}
  \begin{split}
    &\|\T{map f t}\| = 2 \oplus (f_c \otimes t_c) \oplus 1 \oplus_c \\
    &\quadfour \T{rec}(t_p, \T{E} \mapsto \LP 1, \T{E}\RP, \\
    &\quadseven \T{N} \mapsto \LP y, \LP t_0, r_0 \RP \LP t_1, r_1 \RP \RP.  \LP 2 \oplus (f_p\ x)_c \otimes r_{0c} \otimes r_{1c}, \T{N} \LP (f_p\ x)_p, r_{0p}, r_{1p}\RP\RP
\end{split}
\end{equation}

The result is shown in equation \ref{eq:ws_treemap_complexity}.

We interpret trees as the number of \T{N} constructors and the maximum label.
\begin{align*}
  \LB tree \RB &= \mathbb{Z} \times \mathbb{Z} \\
  D_\T{tree} &= \{\ast\} + \mathbb{Z} \times \LB \T{tree} \RB \times \LB \T{tree} \RB \\
  size_\T{tree}(\ast) &= (0, 0) \\
  size_\T{tree}(x, (m_0, n_0), (m_1, n_1)) &= (max(x, m_0, m_1), 1 + n_0 + n_1)
\end{align*}

\begin{figure}
  \label{fig:ws_map_interpretation_derivation}
  \caption{Deriviation of interpretation of \T{map f t}}
\[ \begin{split}
\end{split} \]
\end{figure}
